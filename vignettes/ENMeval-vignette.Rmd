---
title: "ENMeval Vignette"
author: "Robert Muscarella, Jamie M. Kass, and Peter Galante"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ENMeval Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, comment="#>")
```

## Introduction

`ENMeval` is an R package that performs automated runs and evaluations of ecological niche models, and currently only implements Maxent. `ENMeval` was made for those who want to "tune" their models to maximize predictive ability and avoid overfitting, or in other words, optimizing the balance between goodness-of-fit and model complexity. The primary function `ENMevaluate` does all the heavy lifting and returns a table of evaluation statistics, and for each setting combination (here, colloquially: runs), a model object and prediction raster. There are also parameter options for calculating niche overlap between predictions, running in parallel to speed up computation, and more.

## Demo

We're going to start by downloading an occurrence dataset for [*Bradypus variegatus*](https://en.wikipedia.org/wiki/Brown-throated_sloth), the Brown-throated sloth.

```{r occDownload}
library(spocc)
bv <- occ('Bradypus variegatus', 'gbif', limit=50, has_coords=TRUE)  # search GBIF for occurrence data
occs <- bv$gbif$data$Bradypus_variegatus[,2:3]  # get the latitude/coordinates for each locality
occs <- occs[!duplicated(occs),]  # remove duplicate rows
```

Let's now get some climate data from WorldClim. We'll download the 19 bioclimatic variables at 10 arcmin resolution, which is about 20 km across at the equator. These climatic data are based on 50-year averages from 1950-2000. Now's also a good time to load the package, as it includes all the downstream dependencies (`raster`, `dismo`, etc.).

```{r envDownload, warning=FALSE, message=FALSE, fig.width=6, fig.height=8}
library(ENMeval)
# first, grab some predictor rasters in the dismo folder (must have dismo package installed)
files <- list.files(path=paste(system.file(package="dismo"), '/ex', sep=''), pattern='grd', full.names=TRUE )
envs <- stack(files)  # put the rasters into a RasterStack, kind of like a list for rasters
plot(envs[[1]])  # plot first raster in stack, bio1 (Annual Mean Temperature)
points(occs)  # plot all the occurrence points -- notice the stray point in the ocean, we'll take care of that later
```

Next, we will specify the background extent by cropping (or "clipping" in ArcGIS terms) our global predictor variable rasters to a finer-scale region. As our models compare the environment at occurrence localities to the environment at background localities, we need to sample random points from a background extent. To ensure we do not include areas that are suitable for our species but are unoccupied due to limitations like dispersal constraints, we will conservatively define the background extent as an area surrounding our occurrence localities. We do this by buffering a bounding box that includes all occurrence localities. Other methods of background extent delineation, like minimum convex hulls, are more conservative because they better characterize the geographic space holding the points.

```{r backgExt, message=FALSE}
occs.sp <- SpatialPoints(occs)  # make a SpatialPoints object
bb <- bbox(occs.sp)  # get the bounding box of the points
bb.buf <- extent(bb[1]-1, bb[3]+1, bb[2]-1, bb[4]+1)  # add 1 degree to each bound (buffer)
envs.backg <- crop(envs, bb.buf)  # crop envs for study extent

# however, we want to remove the Caribbean islands from our extent
library(maptools)
data(wrld_simpl)  # get a simple world countries polygon
ca.sa <- wrld_simpl[wrld_simpl@data$SUBREGION==5|wrld_simpl@data$SUBREGION==13,]  # get polygons for Central and South America
occs.sp@proj4string <- crs(ca.sa)  # both spatial objects have same geographic coordinate system, so just name the CRS for occs with that of wrld_simpl 
occs.sp <- occs.sp[ca.sa]  # run a spatial intersect operation to subset occs to only the points that fall within our polygon
occs <- occs.sp@coords  # rename occs with the new subset
envs.backg <- mask(envs.backg, ca.sa)  # mask envs by this polygon
plot(envs.backg[[1]])
points(occs)  # this is our background extent
```

Here we sample our background points.

```{r backgPts, fig.width=3, fig.height=4}
bg <- randomPoints(envs.backg[[1]], n=10000)  # randomly sample 10,000 background points from one background extent raster (only one per cell without replacement)
plot(envs.backg[[1]])  
points(bg, col='red')  # notice how we have pretty good coverage
```

## Occurrence Partitioning
A run of ENMevaluate begins by using one of six methods to partition occurrence localities into testing and training bins (folds) for k-fold cross-validation (Fielding and Bell 1997; Peterson et al. 2011). Generally, the data partitioning step is done within the main 'ENMevaluate' function call.  In this section, we illustrate the different options here. 

### 1A) Block
The first, three partitioning methods are variations of what Radosavljevic and Anderson (2014) referred to as 'masked geographically structured' data partitioning. Basically, these methods partition both occurrence records and background points into evaluation bins based on some spatial rules. The methods are designed to reduce spatial-autocorrelation between points included in testing and training bins, which can overinflate model performance. 

``` {r part.block}
# First, the 'block' method partitions data according to the latitude and longitude lines that divide the occurrence localities into four bins of (insofar as possible) equal numbers.
# Both occurrence and background localities are assigned to each of the four bins based on their position with respect to these lines. 
blocks <- get.block(occs, bg)

# The resulting object is a list of two vectors that supply the bin designation for each occurrence and background point.
str(blocks)

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=blocks$occ.grp)
```

### 1B) Checkerboard1
ENMeval also includes two variants of a 'checkerboard' approach to partition occurrence localities. These generate checkerboard grids across the study extent that partition the localities into bins. In contrast to the block method, the checkerboard methods subdivide geographic space equally but do not ensure a balanced number of occurrence localities in each bin.
For these methods, the user needs to provide a raster layer on which to base the underlying checkerboard pattern. Here we simply use the predictor variable RasterStack. Additionally, the user needs to define an 'aggregation.factor'. This value tells the number of grids cells to aggregate when making the underlying checkerboard pattern.

``` {r part.ck}
check1 <- get.checkerboard1(occs, envs, bg, aggregation.factor=5)
plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=check1$occ.grp)

# The partitioning method is more clearly illustrated by looking at the background points:
points(bg, pch=21, bg=check1$bg.grp)

# We can change the aggregation factor to better illustrate how this partitioning method works:
check1.large <- get.checkerboard1(occs, envs, bg, aggregation.factor=30)
plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=21, bg=check1.large$bg.grp)
points(occs, pch=21, bg=check1.large$occ.grp, col='white', cex=1.5)

###############################
###### 1C) Checkerboard2 ######
###############################
# The second checkerboard method is partitions the data into k=4 bins.  
# This is done by aggregating the input raster at 2 scales.
# Points are assigned to bins with respect to where they fall in checkerboards of both scales.

check2 <- get.checkerboard2(occs, envs, bg, aggregation.factor=c(5,5))
plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=21, bg=check2$bg.grp)
points(occs, pch=21, bg=check2$occ.grp, col='white', cex=1.5)
```

### 1D) User-defined
Finally, for maximum flexibility, users can define a priori partitions. This provides a flexible way to conduct spatially independent cross validation with background masking. For example, perhaps we would like to partition points based on a k-means clustering routine.

``` {r part.user}
ngrps <- 10
kmeans <- kmeans(occs, ngrps)
occ.grp <- kmeans$cluster

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=occ.grp)

# When using the user-defined partitioning method, we need to supply ENMevaluate with group identifiers for both occurrence points AND background points.
# If we want to use all background points for each group, we can set the background to zero.

bg.grp <- rep(0, nrow(bg))
points(bg, pch=16, bg=bg.grp)

# Alternatively, we may think of various ways to partition background data. 
# This depends on the goals of the study but we might, for example, find it reasonable to partition background by clustering around the centroids of the occurrence clusters.

centers <- kmeans$center
d <- pointDistance(bg, centers, lonlat=T)
bg.grp <- apply(d, 1, function(x) which(x == min(x)))

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=21, bg=bg.grp)
```

### 1E) k-1 Jackknife
The last two partitioning methods do not partition the background points into different groups. Note that neither of these methods accounts for spatial autocorrelation between testing and training localities, which can inflate evaluation metrics, at least for data sets that result from biased sampling (Veloz 2009; Hijmans 2012; Wenger and Olden 2012). 
Primarily when working with small data sets (e.g. < ca. 25 localities), users may choose a special case of k-fold cross-validation where the number of bins (k) is equal to the number of occurrence localities (n) in the data set (Pearson et al. 2007; Shcheglovitova and Anderson 2013). This is referred to as the k-1 jackknife.

``` {r part.jk}
jack <- get.jackknife(occs, bg)
plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=jack$occ.grp)  # note that colors are repeated here
```

### 1F) Random k-fold
Finally, the 'random k-fold' method partitions occurrence localities randomly into a userspecified number of (k) bins. This method is equivalent to the 'cross-validate' partitioning scheme available in the current version of the MAXENT software.

``` {r part.rand}
# For instance, let's partition the data into five evaluation bins:
random <- get.randomkfold(occs, bg, k=6)

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=random$occ.grp)
```

Choosing among the data partitioning methods depends on the research objectives and the characteristics of the study system. We refer to XX for additional considerations on appropriate partitioning for evaluation.

## Running ENMeval
Once you decide which method of data partitioning you would like to use, you are ready to start building models. We move on to the main function in ENMeval: ENMevaluate.

#### Initial considerations:
Unless you supply the function with background points (which is recommended in many cases), you need to define how many background points should be used with the 'n.bg' argument. If any of your predictor variables are categorical values, you need to define which layer(s) in the 'categoricals' argument.

The two main parameters to define when calling ENMevaluate are (1) the range of regularization multiplier values and (2) the combinations of feature class to consider. The regularization multiplier (RM) determines the penalty for adding parameters to the model. Higher RM values impose a stronger penalty on model complexity and thus result in simpler (flatter) model predictions. The feature classes determine the potential shape of the response curves. A model that is only allowed to include linear feature classes will be simpler than a model that is allowed to include all possible feature classes. Much more description of these parameters is available in (CITE OTHER RESOURCES). For the purposes of this vignette, we demonstrate simply how to adjust these parameters. The following section deals with comparing the outputs of each model.

ENMevaluate builds a separate model for each unique combination of RM values and feature class combinations. For example, the following call will build and evaluate 2 models. One with RM=1 and one with RM=2, both allowing only linear features.

``` {r enmeval, hide=c(-1,-5,-11,-16)}
eval1 <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L'))
eval1

# We may, however, want to compare a wider range of models that can use a wider variety of feature classes:
eval2 <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L','LQ','LQP'))
eval2

# When building many models, the command may take a long time to run.
# Of course this depends on the size of your dataset and the computer you are using.
# When working on big projects, running the command in parallel can be faster:
eval2.par <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L','LQ','LQP'), parallel=TRUE)
eval2.par

# Another way to save time at this stage is to turn off the option that generates model predictions across the full study extent (rasterPreds).
# Note that these are needed for calculating AICc values so those are returned as NaN when rasterPreds=F.
eval3 <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L','LQ','LQP'), rasterPreds=FALSE)
eval3

# No predictions generated:
eval3@predictions

# No AICc values calculated
eval3@results$aicc

# We can also calculate one of two niche overlap statistics directly here using the 'niche.overlap' argument, which supports Moran's I or Schoener's D (citations?).
# Note that you can also calculate this value at a later stage using the separate 'calc.niche.overlap' function.
overlap <- calc.niche.overlap(eval2@predictions, stat='D')
overlap

# The 'bin.output' argument determines if separate evaluation statistics for each testing bin are included in the results file.
```

## Exploring the ENMeval results object
UNDER CONSTRUCTION
``` {r stuff}
# let's use "eval2" as our example ENMeval results object
res <- eval2@results  # isolate the results table 
res # examine tuning results--this has information about omission rates, AUC and AICc scores
res[res$delta.AICc==0,]  # find the model settings that resulted in delta.AICc of 0

# access a RasterStack of the model predictions
eval2@predictions

# let's plot the optimal model according to AICc (model with delta AICc equal to 0). Notice that this model was the 10th one that we ran (row 4), which means that it will be the 10th item in our RasterStack
plot(eval2@predictions[[4]])
```

We can also access the list of model objects. You can subset this list using double brackets: e.g. `results@eval2[[2]]`. This will also allow you to access various elements of the model. This model object can also be used for predicting the model into other time periods or geographic areas. The html file no longer exists for these, but other important information can be accessed. Let's take a look at the AICc optimal model object (item 10 again).

```{r mod.obj}
opt <- eval2@models[[4]]  # the above model was in the 4th row of the results table, so the 4th model will be our optimal model
opt
opt@lambdas # "lambdas" file to find which variables were used (see CITATION)
head(opt@results) # "results" shows the Maxent model statistics

head(eval2@occ.pts)  # this simply shows you the data.frame of occurrence records that you used
head(eval2@occ.grp, n=50)  # this shows you which group each of you occurrence records was sorted into based on your grouping method
eval2@partition.method  # which occurrence partitioning method you used
head(eval2@bg.pts)  # coordinates of the background points used
head(eval2@bg.grp, n=50)  # vector of background point groups
```

## Plotting results
UNDER CONSTRUCTION

Plot complex versus smooth models to see effect
Talk about evaluation metrics (table from paper)
Options for selecting optimal models
Plotting response curves (Use response(ENMeval_results@models[[1]]))

### Plotting results
ENMeval has a built-in plotting function to visualize the results of different models. It requires the results table of the ENMevaluation object. By default, it plots delta.AICc values.

``` {r plot.res}
eval.plot(eval2@results)

# you can choose which evaluation metric to plot, and you can include error bars if relevant:
eval.plot(res, 'Mean.AUC', var='Var.AUC')
eval.plot(res, 'Mean.ORmin', var='Var.ORmin')
```

### Plotting model predictions
If you generated raster predictions of the models, you can plot them easily. For example, let's look at the first two models included in our analysis. Notice that the output values are in Maxent's 'raw' units.

``` {r plot.pred}
plot(eval2@predictions[[1]])
# The original input data is stored in the ENMevaluate object so we can easily add the occurrence and background points, colored by evaluation bins: 
points(eval2@bg.pts, pch=3, col=eval2@bg.grp, cex=0.5)
points(eval2@occ.pts, pch=21, bg=eval2@occ.grp)
```

## Downstream
UNDER CONSTRUCTION
Extracting model results from object (various threshold)
Use model object to make a new prediction if you want a logistic prediction
Make a projection to a new extent
Do MESS map (Use mess() is dismo)