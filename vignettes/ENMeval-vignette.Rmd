---
title: "ENMeval Vignette"
author: "Robert Muscarella, Jamie M. Kass, and Peter Galante"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ENMeval Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, comment="#>")
```

## Introduction

[`ENMeval`](https://cran.r-project.org/web/packages/ENMeval/index.html) is an R package that performs automated runs and evaluations of ecological niche models, and currently only implements [Maxent](https://www.cs.princeton.edu/~schapire/maxent/). `ENMeval` was made for those who want to "tune" their models to maximize predictive ability and avoid overfitting, or in other words, optimizing the balance between goodness-of-fit and model complexity. The primary function, `ENMevaluate`, does all the heavy lifting and returns a table of evaluation statistics, and for each setting combination (here, colloquially: *runs*), a model object and a raster showing the model prediction across the study extent. There are also options for calculating niche overlap between predictions, running in parallel to speed up computation, and more. For a more detailed description of the package, check out the open-access publication:

[Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods Ecol Evol, 5: 1198â€“1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

## Demo
In this vignette, we briefly demonstrate acquisition and pre-processing of input data for `ENMeval`. There are a number of other excellent tutorials on these steps, some of which we compiled in the **Resources** section.
We're going to start by downloading an occurrence dataset for [*Bradypus variegatus*](https://en.wikipedia.org/wiki/Brown-throated_sloth), the Brown-throated sloth.

```{r occDownload}
library(spocc)
bv <- occ('Bradypus variegatus', 'gbif', limit=50, has_coords=TRUE)  # search GBIF for occurrence data
occs <- bv$gbif$data$Bradypus_variegatus[,2:3]  # get the latitude/coordinates for each locality
occs <- occs[!duplicated(occs),]  # remove duplicate rows
```

We are going to model the climatic niche suitability for our focal species using climate data from [WorldClim](http://www.worldclim.org/). WorldClim has a range of variables available at various resolutions; for simplicity, here we'll use the 9 bioclimatic variables at 10 arcmin resolution (about 20 km across at the equator) included in the `dismo` package. These climatic data are based on 50-year averages from 1950-2000. Now's also a good time to load the package, as it includes all the downstream dependencies (`raster`, `dismo`, etc.).

```{r envDownload, warning=FALSE, message=FALSE, fig.width=6, fig.height=8}
library(ENMeval)
# first, load some predictor rasters from the dismo folder
files <- list.files(path=paste(system.file(package="dismo"), '/ex', sep=''), pattern='grd', full.names=TRUE )
envs <- stack(files)  # put the rasters into a RasterStack, kind of like a list for rasters
plot(envs[[1]])  # plot first raster in the stack: bio1 (Annual Mean Temperature)
points(occs)  # plot all the occurrence points -- notice the stray point in the ocean, we'll take care of that later
```

Next, we will specify the background extent by cropping (or "clipping" in ArcGIS terms) our global predictor variable rasters to a smaller region. Since our models will compare the environment at occurrence (or, presence) localities to the environment at background localities, we need to sample random points from a background extent. To help ensure we don't include areas that are suitable for our species but are unoccupied due to limitations like dispersal constraints, we will conservatively define the background extent as an area surrounding our occurrence localities. We will do this by buffering a bounding box that includes all occurrence localities. Some other methods of background extent delineation (e.g., minimum convex hulls) are more conservative because they better characterize the geographic space holding the points. In any case, this is one of the many things that you will need to carefully consider for your own study.

```{r backgExt, message=FALSE}
occs.sp <- SpatialPoints(occs)  # make a SpatialPoints object
bb <- bbox(occs.sp)  # get the bounding box of the points
bb.buf <- extent(bb[1]-1, bb[3]+1, bb[2]-1, bb[4]+1)  # add 1 degree to each bound (buffer)
envs.backg <- crop(envs, bb.buf)  # crop envs for study extent
```

We may also, however, want to remove the Caribbean islands (for example) from our background extent. For this, we can use tools from the `maptools` package, which is not loaded with `ENMeval`.

```{r backgExt, message=FALSE}
library(maptools)
data(wrld_simpl)  # get a simple world countries polygon
ca.sa <- wrld_simpl[wrld_simpl@data$SUBREGION==5|wrld_simpl@data$SUBREGION==13,]  # get polygons for Central and South America
occs.sp@proj4string <- crs(ca.sa)  # both spatial objects have same geographic coordinate system, so just name the CRS for occs with that of wrld_simpl 
occs.sp <- occs.sp[ca.sa]  # run a spatial intersect operation to subset occs to only the points that fall within our polygon
occs <- occs.sp@coords  # rename occs with the new subset
envs.backg <- mask(envs.backg, ca.sa)  # mask envs by this polygon
plot(envs.backg[[1]])
points(occs)  # this is our background extent
```

In the next step, we'll sample 10,000 random points from the background (note that the number of background points is also a consideration you should make with respect to your own study).

```{r backgPts, fig.width=3, fig.height=4}
bg <- randomPoints(envs.backg[[1]], n=10000)  # randomly sample 10,000 background points from one background extent raster (only one per cell without replacement)
plot(envs.backg[[1]])  
points(bg, col='red')  # notice how we have pretty good coverage
```

## Partitioning Occurrences for Evaluation
A run of ENMevaluate begins by using one of six methods to partition occurrence localities into testing and training bins (folds) for k-fold cross-validation (Fielding and Bell 1997; Peterson et al. 2011). Generally, the data partitioning step is done within the main 'ENMevaluate' function call.  In this section, we illustrate the different options. 

The first three partitioning methods are variations of what Radosavljevic and Anderson (2014) referred to as 'masked geographically structured' data partitioning. Basically, these methods partition both occurrence records and background points into evaluation bins based on some spatial rules. The intention is to reduce spatial-autocorrelation between points that are included in the testing and training bins, which can overinflate model performance, at least for data sets that result from biased sampling (Veloz 2009; Hijmans 2012; Wenger and Olden 2012). 

#### 1) Block
First, the 'block' method partitions data according to the latitude and longitude lines that divide the occurrence localities into four bins of (insofar as possible) equal numbers. Both occurrence and background localities are assigned to each of the four bins based on their position with respect to these lines. The resulting object is a list of two vectors that supply the bin designation for each occurrence and background point.

``` {r part.block}
blocks <- get.block(occs, bg)

str(blocks)

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=blocks$occ.grp)
```

#### 2) Checkerboard1
The next two partitioning methods are variants of a 'checkerboard' approach to partition occurrence localities. These generate checkerboard grids across the study extent and partition the localities into bins based on where they fall in the checkerboard. In contrast to the block method, both checkerboard methods subdivide geographic space equally but do not ensure a balanced number of occurrence localities in each bin. For these methods, the user needs to provide a raster layer on which to base the underlying checkerboard pattern. Here we simply use the predictor variable RasterStack. Additionally, the user needs to define an *aggregation.factor*. This value tells the number of grids cells to aggregate when making the underlying checkerboard pattern.

The Checkerboard1 method partitions the points into k=2 bins using a simple checkerboard pattern.

``` {r part.ck}
check1 <- get.checkerboard1(occs, envs, bg, aggregation.factor=5)

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=check1$occ.grp)

# The partitioning method is more clearly illustrated by looking at the background points:
points(bg, pch=21, bg=check1$bg.grp)

# We can change the aggregation factor to better illustrate how this partitioning method works:
check1.large <- get.checkerboard1(occs, envs, bg, aggregation.factor=30)
plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=21, bg=check1.large$bg.grp)
points(occs, pch=21, bg=check1.large$occ.grp, col='white', cex=1.5)
```
#### 3) Checkerboard2
The Checkerboard2 method partitions the data into k=4 bins. This is done by aggregating the input raster at two scales. Presence and background points are assigned to a bin with respect to where they fall in checkerboards of both scales.

``` {r part.ck}
check2 <- get.checkerboard2(occs, envs, bg, aggregation.factor=c(5,5))

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=21, bg=check2$bg.grp)
points(occs, pch=21, bg=check2$occ.grp, col='white', cex=1.5)
```

#### 4) k-1 Jackknife
The next two methods differ from the first three in that (i) they do not partition the background points into different groups, and (ii) they do not account for spatial autocorrelation between testing and training localities. Primarily when working with relatively small data sets (e.g. < ca. 25 presence localities), users may choose a special case of k-fold cross-validation where the number of bins (k) is equal to the number of occurrence localities (n) in the data set (Pearson et al. 2007; Shcheglovitova and Anderson 2013). This is referred to as the k-1 jackknife.  This method will take prohibitively long times for computation when the number of presence localities is medium to large.

``` {r part.jk}
jack <- get.jackknife(occs, bg)

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=jack$occ.grp)  # note that colors are repeated here
```

#### 5) Random k-fold
The 'random k-fold' method partitions occurrence localities randomly into a userspecified number of (k) bins. This method is equivalent to the 'cross-validate' partitioning scheme available in the current version of the Maxent software GUI.

``` {r part.rand}
# For instance, let's partition the data into five evaluation bins:
random <- get.randomkfold(occs, bg, k=5)

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=random$occ.grp)
```

#### 6) User-defined
For maximum flexibility, the last partitioning method is designed so that users can define *a priori* partitions. This provides a flexible way to conduct spatially-independent cross-validation with background masking. For example, perhaps we would like to partition points based on a k-means clustering routine.

``` {r part.user}
ngrps <- 10
kmeans <- kmeans(occs, ngrps)
occ.grp <- kmeans$cluster

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(occs, pch=21, bg=occ.grp)
```

When using the user-defined partitioning method, we need to supply ENMevaluate with group identifiers for both occurrence points AND background points. If we want to use all background points for each group, we can set the background to zero.

``` {r part.user}
bg.grp <- rep(0, nrow(bg))

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=16, bg=bg.grp)
```
Alternatively, we may think of various ways to partition background data. This depends on the goals of the study but we might, for example, find it reasonable to partition background by clustering around the centroids of the occurrence clusters.

``` {r part.user}
centers <- kmeans$center
d <- pointDistance(bg, centers, lonlat=T)
bg.grp <- apply(d, 1, function(x) which(x == min(x)))

plot(envs.backg[[1]], col='gray', legend=FALSE)
points(bg, pch=21, bg=bg.grp)
```

Choosing among these data partitioning methods depends on the research objectives and the characteristics of the study system. Refer to the **Resources** section for additional considerations on appropriate partitioning for evaluation.

## Running ENMeval
Once you decide which method of data partitioning you would like to use, you are ready to start building models. We now move on to the main function in ENMeval: `ENMevaluate`.

#### Initial considerations:
The two main parameters to define when calling `ENMevaluate` are (1) the range of regularization multiplier values and (2) the combinations of feature class to consider. The ***regularization multiplier*** (RM) determines the penalty for adding parameters to the model. Higher RM values impose a stronger penalty on model complexity and thus result in simpler (*flatter*) model predictions. The ***feature classes*** determine the potential shape of the response curves. A model that is only allowed to include linear feature classes will most likely be simpler than a model that is allowed to include all possible feature classes. Much more description of these parameters is available in the **Resources** section. For the purposes of this vignette, we demonstrate simply how to adjust these parameters. The following section deals with comparing the outputs of each model.

Unless you supply the function with background points (which is recommended in many cases), you will need to define how many background points should be used with the 'n.bg' argument. If any of your predictor variables are categorical (e.g., biomes), you will need to define which layer(s) these are using the 'categoricals' argument.

ENMevaluate builds a separate model for each unique combination of RM values and feature class combinations. For example, the following call will build and evaluate 2 models. One with RM=1 and one with RM=2, both allowing only linear features.

``` {r enmeval, hide=c(-1,-5,-11,-16)}
eval1 <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L'))
eval1

# We may, however, want to compare a wider range of models that can use a wider variety of feature classes:
eval2 <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L','LQ','LQP'))
eval2
```

When building many models, the command may take a long time to run. Of course this depends on the size of your dataset and the computer you are using. When working on big projects, running the command in parallel can be faster.

``` {r enmeval, hide=c(-1,-5,-11,-16)}
eval2.par <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L','LQ','LQP'), parallel=TRUE)
eval2.par
```

Another way to save time at this stage is to turn off the option that generates model predictions across the full study extent (rasterPreds). Note, however, that these are needed for calculating AICc values so those are returned as NA when the `rasterPreds` argument is set to FALSE.

``` {r enmeval, hide=c(-1,-5,-11,-16)}
eval3 <- ENMevaluate(occs, envs, bg, method='checkerboard2', RMvalues=c(1,2), fc=c('L','LQ','LQP'), rasterPreds=FALSE)
eval3

# Note that no predictions were generated:
eval3@predictions

# And no AICc values calculated:
eval3@results$aicc
```

We can also calculate one of two niche overlap statistics while running `ENMevaluate` by setting the `niche.overlap` argument, which supports Moran's I or Schoener's D. Note that you can also calculate this value at a later stage using the separate `calc.niche.overlap` function.

``` {r enmeval, hide=c(-1,-5,-11,-16)}
overlap <- calc.niche.overlap(eval2@predictions, stat='D')
overlap
```

The 'bin.output' argument determines if separate evaluation statistics for each testing bin are included in the results file.


## Exploring the ENMeval results object
UNDER CONSTRUCTION
``` {r stuff}
# let's use "eval2" as our example ENMeval results object
res <- eval2@results  # isolate the results table 
res # examine tuning results--this has information about omission rates, AUC and AICc scores
res[res$delta.AICc==0,]  # find the model settings that resulted in delta.AICc of 0

# access a RasterStack of the model predictions
eval2@predictions

# let's plot the optimal model according to AICc (model with delta AICc equal to 0). Notice that this model was the 10th one that we ran (row 4), which means that it will be the 10th item in our RasterStack
plot(eval2@predictions[[4]])
```

We can also access the list of model objects. You can subset this list using double brackets: e.g. `results@eval2[[2]]`. This will also allow you to access various elements of the model. This model object can also be used for predicting the model into other time periods or geographic areas. The html file no longer exists for these, but other important information can be accessed. Let's take a look at the AICc optimal model object (item 10 again).

```{r mod.obj}
opt <- eval2@models[[4]]  # the above model was in the 4th row of the results table, so the 4th model will be our optimal model
opt
opt@lambdas # "lambdas" file to find which variables were used (see CITATION)
head(opt@results) # "results" shows the Maxent model statistics

head(eval2@occ.pts)  # this simply shows you the data.frame of occurrence records that you used
head(eval2@occ.grp, n=50)  # this shows you which group each of you occurrence records was sorted into based on your grouping method
eval2@partition.method  # which occurrence partitioning method you used
head(eval2@bg.pts)  # coordinates of the background points used
head(eval2@bg.grp, n=50)  # vector of background point groups
```

## Plotting results
UNDER CONSTRUCTION

Plot complex versus smooth models to see effect
Talk about evaluation metrics (table from paper)
Options for selecting optimal models
Plotting response curves (Use response(ENMeval_results@models[[1]]))

### Plotting results
ENMeval has a built-in plotting function to visualize the results of different models. It requires the results table of the ENMevaluation object. By default, it plots delta.AICc values.

``` {r plot.res}
eval.plot(eval2@results)

# you can choose which evaluation metric to plot, and you can include error bars if relevant:
eval.plot(res, 'Mean.AUC', var='Var.AUC')
eval.plot(res, 'Mean.ORmin', var='Var.ORmin')
```

### Plotting model predictions
If you generated raster predictions of the models, you can plot them easily. For example, let's look at the first two models included in our analysis. Notice that the output values are in Maxent's 'raw' units.

``` {r plot.pred}
plot(eval2@predictions[[1]])
# The original input data is stored in the ENMevaluate object so we can easily add the occurrence and background points, colored by evaluation bins: 
points(eval2@bg.pts, pch=3, col=eval2@bg.grp, cex=0.5)
points(eval2@occ.pts, pch=21, bg=eval2@occ.grp)
```

## Downstream
UNDER CONSTRUCTION
Extracting model results from object (various threshold)
Use model object to make a new prediction if you want a logistic prediction
Make a projection to a new extent
Do MESS map (Use mess() is dismo)
